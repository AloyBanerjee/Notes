{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAIPKWqyWTf6",
        "outputId": "4ecff88a-2ed7-4788-89f4-bbf4cfd8fa8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=bc06891a1a3c98c0fec22ff88b051d2c601f7604b0bff9f7643f8e1af7077040\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8X8bVEXWpjL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Streaming\").config('spark.ui.port', '4050').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtnHEfjLpzO_",
        "outputId": "9d10cb24-a27f-444d-fd2d-e16adbadfa64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-10 17:50:21--  https://raw.githubusercontent.com/lawlesst/vivo-sample-data/master/data/csv/people.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4095 (4.0K) [text/plain]\n",
            "Saving to: ‘people.csv’\n",
            "\n",
            "\rpeople.csv            0%[                    ]       0  --.-KB/s               \rpeople.csv          100%[===================>]   4.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-10 17:50:21 (64.4 MB/s) - ‘people.csv’ saved [4095/4095]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/lawlesst/vivo-sample-data/master/data/csv/people.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v3yCvMWkoKZ"
      },
      "source": [
        "Copy a CSV file into a folder, convert it to JSON, and write it to the output folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n5Hbh7qkmcE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Building a spark session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Streaming\").config('spark.ui.port', '4050').getOrCreate()\n",
        "\n",
        "# Copy the CSV file to the input folder\n",
        "!mkdir input/\n",
        "!cp people.csv input/\n",
        "\n",
        "# Read the CSV file from the input folder\n",
        "people_df = spark.read.format(\"csv\").option(\"header\", True).load(\"input/people.csv\")\n",
        "\n",
        "# Write the DataFrame as JSON to the output folder\n",
        "people_df.write.format(\"json\").mode(\"overwrite\").save(\"output/people_json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "people_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5viQgjwrUkx",
        "outputId": "73d341e7-a633-4ae6-b018-0ed885070b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+---------+--------+--------+------------------+-----------------+------------+-------------------+\n",
            "|person_ID|                name|    first|    last|  middle|             email|            phone|         fax|              title|\n",
            "+---------+--------------------+---------+--------+--------+------------------+-----------------+------------+-------------------+\n",
            "|     3130|     Burks, Rosella |  Rosella|   Burks|    null|   BurksR@univ.edu|     963.555.1253|963.777.4065|         Professor |\n",
            "|     3297|      Avila, Damien |   Damien|   Avila|    null|   AvilaD@univ.edu|     963.555.1352|963.777.7914|         Professor |\n",
            "|     3547|       Olsen, Robin |    Robin|   Olsen|    null|   OlsenR@univ.edu|     963.555.1378|963.777.9262|Assistant Professor|\n",
            "|     1538| Moises, Edgar Estes|    Edgar|  Moises|   Estes|  MoisesE@univ.edu|963.555.2731x3565|963.777.8264|          Professor|\n",
            "|     2941| Brian, Heath Pruitt|    Heath|   Brian|  Pruitt|   BrianH@univ.edu|     963.555.2800|963.777.7249| Associate Curator |\n",
            "|     2401| Claude, Elvin Haney|    Elvin|  Claude|   Haney|  ClaudeE@univ.edu|     963.555.2902|963.777.9730|            Curator|\n",
            "|     2070|     Mosley, Edmund |   Edmund|  Mosley|    null|  MosleyE@univ.edu|     963.555.2945|963.777.9285|Assistant Professor|\n",
            "|     2561|Derek, Antoine Mccoy|  Antoine|   Derek|   Mccoy|   DerekA@univ.edu|     963.555.2992|963.777.5454|            Curator|\n",
            "|     1625|    Hawkins, Callie |   Callie| Hawkins|    null| HawkinsC@univ.edu|963.555.3350x6480|963.777.4949|          Professor|\n",
            "|     1307|       Pate, Andrea |   Andrea|    Pate|    null|    PateA@univ.edu|     963.555.3723|963.777.5049|          Professor|\n",
            "|     2342|        Austin, Liz |      Liz|  Austin|    null|  AustinL@univ.edu|     963.555.4305|963.777.6143|Assistant Professor|\n",
            "|     2755|Kendrick, Reba Al...|     Reba|Kendrick|  Alford|KendrickR@univ.edu|963.555.4618x7744|963.777.4371| Associate Curator |\n",
            "|     4150|     Sims, Angelina | Angelina|    Sims|    null|    SimsA@univ.edu|     963.555.5278|963.777.4400|          Professor|\n",
            "|     3544|  Mullins, Kimberly | Kimberly| Mullins|    null| MullinsK@univ.edu|963.555.5471x1017|963.777.9783|Assistant Professor|\n",
            "|     2096|  Chuck, Lloyd Haney|    Lloyd|   Chuck|   Haney|   ChuckL@univ.edu|963.555.5568x2652|963.777.9290|Assistant Professor|\n",
            "|     1089|     Payne, Ladonna |  Ladonna|   Payne|    null|   PayneL@univ.edu|     963.555.5639|963.777.6469|          Professor|\n",
            "|     2948|Baxter, Johnathan...|Johnathan|  Baxter|Browning|  BaxterJ@univ.edu|     963.555.5902|963.777.8336| Research Professor|\n",
            "|     4539|     Weiss, Gilbert |  Gilbert|   Weiss|    null|   WeissG@univ.edu|     963.555.5969|963.777.4975|          Professor|\n",
            "|     2811|Deirdre, Florence...| Florence| Deirdre| Barrera| DeirdreF@univ.edu|     963.555.6319|963.777.9654| Associate Curator |\n",
            "|     4580|Fernando, Toby Ca...|     Toby|Fernando|Calderon|FernandoT@univ.edu|     963.555.6469|963.777.9864| Research Professor|\n",
            "+---------+--------------------+---------+--------+--------+------------------+-----------------+------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-cmkCgAlg9j"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Run a streaming job to compute an aggregate for every minute window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd1VFzSxqUxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "70d7f912-1472-4581-deed-415cf042daf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+-----+\n",
            "|filename|title|count|\n",
            "+--------+-----+-----+\n",
            "+--------+-----+-----+\n",
            "\n",
            "+---------+----+---------+-----+\n",
            "|person_ID|name|person_ID|title|\n",
            "+---------+----+---------+-----+\n",
            "+---------+----+---------+-----+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-58f8d96f5614>\u001b[0m in \u001b[0;36m<cell line: 107>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m#Code 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mtumblingWindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindowing_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithWatermark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timeReceived\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"10 minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eventId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timeReceived\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"10 minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0mtumblingWindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'windowing_df' is not defined"
          ]
        }
      ],
      "source": [
        "#Code 1\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import input_file_name\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Streaming\").config('spark.ui.port', '4050').getOrCreate()\n",
        "\n",
        "# Define the schema for the streaming DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"person_ID\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"first\", StringType(), True),\n",
        "    StructField(\"last\", StringType(), True),\n",
        "    StructField(\"middle\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"phone\", StringType(), True),\n",
        "    StructField(\"fax\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read the streaming DataFrame from CSV\n",
        "people_df = spark.readStream.format(\"csv\").schema(schema).option(\"header\", True).load(\"input\")\n",
        "people_df = people_df.withColumn(\"filename\",input_file_name())\n",
        "\n",
        "# # Partition the data by the key column for parallel processing\n",
        "# partitioned_people_df = people_df.repartition(\"title\")\n",
        "\n",
        "# Perform the aggregation on the title column\n",
        "title_count_df = people_df.groupBy([\"filename\",\"title\"]).count()\n",
        "\n",
        "# Start the streaming query to compute aggregates every minute\n",
        "query = title_count_df.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .queryName(\"aggregates\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()\n",
        "\n",
        "# Wait for the streaming query to process data\n",
        "# query.awaitTermination()\n",
        "\n",
        "# Display the results of the aggregates\n",
        "spark.sql(\"select * from aggregates order by 1,2\").show(100,truncate=False)\n",
        "\n",
        "#Code 2\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SplitCSVByColumns\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the CSV file\n",
        "csv_df = spark.read.csv(\"input/people.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Select specific columns\n",
        "column1_df = csv_df.select(\"person_ID\", \"name\")\n",
        "column2_df = csv_df.select(\"person_ID\", \"title\")\n",
        "\n",
        "# Save each column as a separate CSV file\n",
        "column1_df.write.csv(\"name.csv\", header=True, mode=\"overwrite\")\n",
        "column2_df.write.csv(\"title.csv\", header=True, mode=\"overwrite\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamingJoin\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema for the streaming DataFrames\n",
        "schema1 = StructType([\n",
        "    StructField(\"person_ID\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True)\n",
        "])\n",
        "\n",
        "schema2 = StructType([\n",
        "    StructField(\"person_ID\", IntegerType(), True),\n",
        "    StructField(\"title\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read the streaming DataFrames\n",
        "df1 = spark.readStream.format(\"csv\").schema(schema1).option(\"header\", True).load(\"name.csv\")\n",
        "df2 = spark.readStream.format(\"csv\").schema(schema2).option(\"header\", True).load(\"title.csv\")\n",
        "\n",
        "# Perform the join operation\n",
        "joined_df = df1.join(df2, df1[\"person_ID\"] == df2[\"person_ID\"])\n",
        "\n",
        "# Select the columns you want to include in the results\n",
        "results_df = joined_df.select(\"*\")\n",
        "\n",
        "# Start the streaming query to output the results\n",
        "query = results_df.writeStream \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .queryName(\"streamingjointitledf\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n",
        "\n",
        "spark.sql(\"select * from streamingjointitledf\").show(truncate=False)\n",
        "\n",
        "#Code 4\n",
        "from pyspark.sql.functions import *\n",
        "tumblingWindows = windowing_df.withWatermark(\"timeReceived\", \"10 minutes\").groupBy(\"eventId\", window(\"timeReceived\", \"10 minutes\")).count()\n",
        "tumblingWindows.show(truncate = False)\n",
        "\n",
        "#Code 5\n",
        "from pyspark.sql.functions import *\n",
        "slidingWindows = windowing_df.withWatermark(\"timeReceived\", \"10 minutes\").groupBy(\"eventId\", window(\"timeReceived\", \"10 minutes\", \"5 minutes\")).count()\n",
        "slidingWindows.show(truncate = False)\n",
        "\n",
        "#Code 6\n",
        "from pyspark.sql.functions import *\n",
        "sessionWindows = windowing_df.withWatermark(\"timeReceived\", \"10 minutes\").groupBy(\"eventId\", session_window(\"timeReceived\", \"5 minutes\")).count()\n",
        "sessionWindows.show(truncate = False)\n",
        "\n",
        "#Code 7\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "windowedCountsDF = windowing_df.withWatermark(\"timeReceived\", \"10 minutes\").groupBy(windowing_df.eventId, session_window(windowing_df.timeReceived, \\\n",
        "                                                                                                                         when(windowing_df.eventId == \"20\", \"10 seconds\").when(windowing_df.eventId == \"12\",\"30 seconds\").otherwise(\"10 minutes\"))).count()\n",
        "\n",
        "windowedCountsDF.show(100, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgxQ34ROt8XK",
        "outputId": "0b562ef4-1331-4e66-9b24-820945be7cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+-------------------+-----+\n",
            "|filename                        |title              |count|\n",
            "+--------------------------------+-------------------+-----+\n",
            "|file:///content/input/people.csv|Assistant Professor|11   |\n",
            "|file:///content/input/people.csv|Associate Curator  |5    |\n",
            "|file:///content/input/people.csv|Curator            |5    |\n",
            "|file:///content/input/people.csv|Professor          |10   |\n",
            "|file:///content/input/people.csv|Professor          |4    |\n",
            "|file:///content/input/people.csv|Research Professor |5    |\n",
            "+--------------------------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the results of the aggregates\n",
        "spark.sql(\"select * from aggregates order by 1,2\").show(100,truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYmLXQBQ9r9s"
      },
      "source": [
        "Split the csvs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K_0kCCDrF4y"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SplitCSVByColumns\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the CSV file\n",
        "csv_df = spark.read.csv(\"input/people.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Select specific columns\n",
        "column1_df = csv_df.select(\"person_ID\", \"name\")\n",
        "column2_df = csv_df.select(\"person_ID\", \"title\")\n",
        "\n",
        "# Save each column as a separate CSV file\n",
        "column1_df.write.csv(\"name.csv\", header=True, mode=\"overwrite\")\n",
        "column2_df.write.csv(\"title.csv\", header=True, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x_taPSy95Cx"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamingJoin\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema for the streaming DataFrames\n",
        "schema1 = StructType([\n",
        "    StructField(\"person_ID\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True)\n",
        "])\n",
        "\n",
        "schema2 = StructType([\n",
        "    StructField(\"person_ID\", IntegerType(), True),\n",
        "    StructField(\"title\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read the streaming DataFrames\n",
        "df1 = spark.readStream.format(\"csv\").schema(schema1).option(\"header\", True).load(\"name.csv\")\n",
        "df2 = spark.readStream.format(\"csv\").schema(schema2).option(\"header\", True).load(\"title.csv\")\n",
        "\n",
        "# Perform the join operation\n",
        "joined_df = df1.join(df2, df1[\"person_ID\"] == df2[\"person_ID\"])\n",
        "\n",
        "# Select the columns you want to include in the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAw4GDRgFY5b"
      },
      "outputs": [],
      "source": [
        "results_df = joined_df.select(\"*\")\n",
        "\n",
        "# Start the streaming query to output the results\n",
        "query = results_df.writeStream \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .queryName(\"streamingjointitledf\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n",
        "\n",
        "# Wait for the query to terminate\n",
        "# query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynk0VwoS-8dE",
        "outputId": "6fb5bddc-ee35-4bc7-d4af-504c1f0184a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+---------+-------------------+\n",
            "|person_ID|name                     |person_ID|title              |\n",
            "+---------+-------------------------+---------+-------------------+\n",
            "|2811     |Deirdre, Florence Barrera|2811     |Associate Curator  |\n",
            "|1699     |Stanton, Kathie          |1699     |Professor          |\n",
            "|2096     |Chuck, Lloyd Haney       |2096     |Assistant Professor|\n",
            "|2342     |Austin, Liz              |2342     |Assistant Professor|\n",
            "|2682     |Earline, Jaime Fitzgerald|2682     |Associate Curator  |\n",
            "|1307     |Pate, Andrea             |1307     |Professor          |\n",
            "|1824     |Head, Kurtis             |1824     |Professor          |\n",
            "|3095     |Shields, Rich Pena       |3095     |Professor          |\n",
            "|2941     |Brian, Heath Pruitt      |2941     |Associate Curator  |\n",
            "|2383     |Page, Winnie             |2383     |Curator            |\n",
            "|1089     |Payne, Ladonna           |1089     |Professor          |\n",
            "|1538     |Moises, Edgar Estes      |1538     |Professor          |\n",
            "|1968     |Michael, Dianne          |1968     |Assistant Professor|\n",
            "|3012     |Grant, Adam              |3012     |Research Professor |\n",
            "|2755     |Kendrick, Reba Alford    |2755     |Associate Curator  |\n",
            "|2895     |Garrison, Patrica        |2895     |Associate Curator  |\n",
            "|3112     |Evelyn, Summer Frost     |3112     |Professor          |\n",
            "|2401     |Claude, Elvin Haney      |2401     |Curator            |\n",
            "|3066     |Barnes, Cleo             |3066     |Research Professor |\n",
            "|3958     |Kaufman, Elba            |3958     |Professor          |\n",
            "+---------+-------------------------+---------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from streamingjointitledf\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h0ObEm3uPck",
        "outputId": "3e11a1f2-fea1-4dfb-b4a1-5aeffaeda468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- eventId: string (nullable = true)\n",
            " |-- timeReceived: string (nullable = true)\n",
            "\n",
            "+-------+-------------------+\n",
            "|eventId|timeReceived       |\n",
            "+-------+-------------------+\n",
            "|12     |2019-01-02 15:30:00|\n",
            "|12     |2019-01-02 15:30:30|\n",
            "|12     |2019-01-02 15:31:00|\n",
            "|12     |2019-01-02 15:31:50|\n",
            "|12     |2019-01-02 15:31:55|\n",
            "|16     |2019-01-02 15:33:00|\n",
            "|16     |2019-01-02 15:35:20|\n",
            "|16     |2019-01-02 15:37:00|\n",
            "|20     |2019-01-02 15:30:30|\n",
            "|20     |2019-01-02 15:31:00|\n",
            "|20     |2019-01-02 15:31:50|\n",
            "|20     |2019-01-02 15:31:55|\n",
            "|20     |2019-01-02 15:33:00|\n",
            "|20     |2019-01-02 15:35:20|\n",
            "|20     |2019-01-02 15:37:00|\n",
            "|20     |2019-01-02 15:40:00|\n",
            "|20     |2019-01-02 15:45:00|\n",
            "|20     |2019-01-02 15:46:00|\n",
            "|20     |2019-01-02 15:47:30|\n",
            "|20     |2019-01-02 15:48:00|\n",
            "+-------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "windowingData = ((\"12\", \"2019-01-02 15:30:00\"),\n",
        "(\"12\",  \"2019-01-02 15:30:30\"),\n",
        "(\"12\",  \"2019-01-02 15:31:00\"),\n",
        "(\"12\",  \"2019-01-02 15:31:50\"),\n",
        "(\"12\",  \"2019-01-02 15:31:55\"),\n",
        "(\"16\",  \"2019-01-02 15:33:00\"),\n",
        "(\"16\",  \"2019-01-02 15:35:20\"),\n",
        "(\"16\",  \"2019-01-02 15:37:00\"),\n",
        "(\"20\",  \"2019-01-02 15:30:30\"),\n",
        "(\"20\",  \"2019-01-02 15:31:00\"),\n",
        "(\"20\",  \"2019-01-02 15:31:50\"),\n",
        "(\"20\",  \"2019-01-02 15:31:55\"),\n",
        "(\"20\",  \"2019-01-02 15:33:00\"),\n",
        "(\"20\",  \"2019-01-02 15:35:20\"),\n",
        "(\"20\",  \"2019-01-02 15:37:00\"),\n",
        "(\"20\",  \"2019-01-02 15:40:00\"),\n",
        "(\"20\",  \"2019-01-02 15:45:00\"),\n",
        "(\"20\",  \"2019-01-02 15:46:00\"),\n",
        "(\"20\",  \"2019-01-02 15:47:30\"),\n",
        "(\"20\",  \"2019-01-02 15:48:00\"),\n",
        "(\"20\",  \"2019-01-02 15:48:10\"),\n",
        "(\"20\",  \"2019-01-02 15:48:20\"),\n",
        "(\"20\",  \"2019-01-02 15:48:30\"),\n",
        "(\"20\",  \"2019-01-02 15:50:00\"),\n",
        "(\"20\",  \"2019-01-02 15:53:00\"),\n",
        "(\"20\",  \"2019-01-02 15:54:30\"),\n",
        "(\"20\",  \"2019-01-02 15:55:00\"),\n",
        "(\"22\",  \"2019-01-02 15:50:30\"),\n",
        "(\"22\",  \"2019-01-02 15:52:00\"),\n",
        "(\"22\",  \"2019-01-02 15:50:30\"),\n",
        "(\"22\",  \"2019-01-02 15:52:00\"),\n",
        "(\"22\",  \"2019-01-02 15:50:30\"),\n",
        "(\"22\",  \"2019-01-02 15:52:00\"))\n",
        "columns = [\"eventId\", \"timeReceived\"]\n",
        "windowing_df = spark.createDataFrame(data = windowingData, schema = columns)\n",
        "windowing_df.printSchema()\n",
        "windowing_df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO316YvxugpC"
      },
      "source": [
        "# Tumbling window\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*13n52gO_dw2TTxpbNRE9bg.png' />\n",
        "\n",
        "[API](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.window.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxR_VuTVuYvM",
        "outputId": "0a4d2a84-4009-4e7b-e58d-78d9ee1e0d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------+-----+\n",
            "|eventId|window                                    |count|\n",
            "+-------+------------------------------------------+-----+\n",
            "|20     |{2019-01-02 15:30:00, 2019-01-02 15:40:00}|7    |\n",
            "|20     |{2019-01-02 15:40:00, 2019-01-02 15:50:00}|8    |\n",
            "|16     |{2019-01-02 15:30:00, 2019-01-02 15:40:00}|3    |\n",
            "|20     |{2019-01-02 15:50:00, 2019-01-02 16:00:00}|4    |\n",
            "|22     |{2019-01-02 15:50:00, 2019-01-02 16:00:00}|6    |\n",
            "|12     |{2019-01-02 15:30:00, 2019-01-02 15:40:00}|5    |\n",
            "+-------+------------------------------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "tumblingWindows = windowing_df.withWatermark(\"timeReceived\", \"10 minutes\").groupBy(\"eventId\", window(\"timeReceived\", \"10 minutes\")).count()\n",
        "tumblingWindows.show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Yaxu36ul40"
      },
      "source": [
        "# Sliding Window\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*aCXwoamSeMSN1_Sv07M6hg.png' />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNMmrbMoubDM",
        "outputId": "5bca66af-1891-45e7-bc48-a9f1fe4d4b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------+-----+\n",
            "|eventId|window                                    |count|\n",
            "+-------+------------------------------------------+-----+\n",
            "|16     |{2019-01-02 15:25:00, 2019-01-02 15:35:00}|1    |\n",
            "|20     |{2019-01-02 15:30:00, 2019-01-02 15:40:00}|7    |\n",
            "|20     |{2019-01-02 15:55:00, 2019-01-02 16:05:00}|1    |\n",
            "|12     |{2019-01-02 15:25:00, 2019-01-02 15:35:00}|5    |\n",
            "|20     |{2019-01-02 15:40:00, 2019-01-02 15:50:00}|8    |\n",
            "|20     |{2019-01-02 15:25:00, 2019-01-02 15:35:00}|5    |\n",
            "|16     |{2019-01-02 15:30:00, 2019-01-02 15:40:00}|3    |\n",
            "|20     |{2019-01-02 15:50:00, 2019-01-02 16:00:00}|4    |\n",
            "|22     |{2019-01-02 15:45:00, 2019-01-02 15:55:00}|6    |\n",
            "|22     |{2019-01-02 15:50:00, 2019-01-02 16:00:00}|6    |\n",
            "|20     |{2019-01-02 15:45:00, 2019-01-02 15:55:00}|10   |\n",
            "|16     |{2019-01-02 15:35:00, 2019-01-02 15:45:00}|2    |\n",
            "|12     |{2019-01-02 15:30:00, 2019-01-02 15:40:00}|5    |\n",
            "|20     |{2019-01-02 15:35:00, 2019-01-02 15:45:00}|3    |\n",
            "+-------+------------------------------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "slidingWindows = windowing_df.withWatermark(\"timeReceived\", \"10 minutes\").groupBy(\"eventId\", window(\"timeReceived\", \"10 minutes\", \"5 minutes\")).count()\n",
        "slidingWindows.show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT0n-FjTw4WO"
      },
      "source": [
        "# Session window\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PprkRQPbYPSkrZ66LIAzHA.png' />\n",
        "\n",
        "\n",
        "[API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.session_window.html#:~:text=session_window,-pyspark.sql.functions&text=Generates%20session%20window%20given%20a,according%20to%20the%20given%20inputs.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hucuweZgusxw",
        "outputId": "aa2b6de1-253c-42b4-83a7-d8459c40f5c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------+-----+\n",
            "|eventId|session_window                            |count|\n",
            "+-------+------------------------------------------+-----+\n",
            "|12     |{2019-01-02 15:30:00, 2019-01-02 15:36:55}|5    |\n",
            "|16     |{2019-01-02 15:33:00, 2019-01-02 15:42:00}|3    |\n",
            "|20     |{2019-01-02 15:30:30, 2019-01-02 16:00:00}|19   |\n",
            "|22     |{2019-01-02 15:50:30, 2019-01-02 15:57:00}|6    |\n",
            "+-------+------------------------------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "sessionWindows = windowing_df.withWatermark(\"timeReceived\", \"1 minutes\").groupBy(\"eventId\", session_window(\"timeReceived\", \"5 minutes\")).count()\n",
        "sessionWindows.show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dIJt2Mwy-Tk"
      },
      "source": [
        "# Dynamic Gapping Period for Session Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_24zfRCxRZn",
        "outputId": "a73c34d3-61fa-4adf-9647-236b7c1ea2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------+-----+\n",
            "|eventId|session_window                            |count|\n",
            "+-------+------------------------------------------+-----+\n",
            "|12     |{2019-01-02 15:30:00, 2019-01-02 15:31:30}|3    |\n",
            "|12     |{2019-01-02 15:31:50, 2019-01-02 15:32:25}|2    |\n",
            "|16     |{2019-01-02 15:33:00, 2019-01-02 15:47:00}|3    |\n",
            "|20     |{2019-01-02 15:30:30, 2019-01-02 15:30:40}|1    |\n",
            "|20     |{2019-01-02 15:31:00, 2019-01-02 15:31:10}|1    |\n",
            "|20     |{2019-01-02 15:31:50, 2019-01-02 15:32:05}|2    |\n",
            "|20     |{2019-01-02 15:33:00, 2019-01-02 15:33:10}|1    |\n",
            "|20     |{2019-01-02 15:35:20, 2019-01-02 15:35:30}|1    |\n",
            "|20     |{2019-01-02 15:37:00, 2019-01-02 15:37:10}|1    |\n",
            "|20     |{2019-01-02 15:40:00, 2019-01-02 15:40:10}|1    |\n",
            "|20     |{2019-01-02 15:45:00, 2019-01-02 15:45:10}|1    |\n",
            "|20     |{2019-01-02 15:46:00, 2019-01-02 15:46:10}|1    |\n",
            "|20     |{2019-01-02 15:47:30, 2019-01-02 15:47:40}|1    |\n",
            "|20     |{2019-01-02 15:48:00, 2019-01-02 15:48:40}|4    |\n",
            "|20     |{2019-01-02 15:50:00, 2019-01-02 15:50:10}|1    |\n",
            "|20     |{2019-01-02 15:53:00, 2019-01-02 15:53:10}|1    |\n",
            "|20     |{2019-01-02 15:54:30, 2019-01-02 15:54:40}|1    |\n",
            "|20     |{2019-01-02 15:55:00, 2019-01-02 15:55:10}|1    |\n",
            "|22     |{2019-01-02 15:50:30, 2019-01-02 16:02:00}|6    |\n",
            "+-------+------------------------------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "windowedCountsDF = windowing_df.withWatermark(\"timeReceived\", \"10 minutes\").groupBy(windowing_df.eventId, session_window(windowing_df.timeReceived, \\\n",
        "    when(windowing_df.eventId == \"20\", \"10 seconds\").when(windowing_df.eventId == \"12\",\"30 seconds\").otherwise(\"10 minutes\"))).count()\n",
        "\n",
        "windowedCountsDF.show(100, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQA6Bx9Ay8K6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}