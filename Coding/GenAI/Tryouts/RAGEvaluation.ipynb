{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "##### How correct the response is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"The dog chased the cat up the tree, who ran up the tree?\",\n",
    "    actual_output=\"It depends, some might consider the cat, while others might argue the dog.\",\n",
    "    expected_output=\"The cat.\"\n",
    ")\n",
    "correctness_metric.measure(test_case)\n",
    "print(correctness_metric.score)\n",
    "print(correctness_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Alignment\n",
    "##### The prompt alignment metric measures whether your LLM application is able to generate actual_outputs that aligns with any instructions specified in your prompt template. deepeval's prompt alignment metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import PromptAlignmentMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "metric = PromptAlignmentMetric(\n",
    "    prompt_instructions=[\"Reply in all uppercase\"],\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"We offer a 30-day full refund at no extra cost.\"\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevancy\n",
    "##### The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the actual_output of your LLM application is compared to the provided input. deepeval's answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Precision\n",
    "##### The contextual precision metric measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval_context that are relevant to the given input are ranked higher than irrelevant ones. deepeval's contextual precision metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the expected output from your RAG generator\n",
    "expected_output = \"You are eligible for a 30 day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Recall\n",
    "##### The contextual recall metric measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval_context aligns with the expected_output. deepeval's contextual recall metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the expected output from your RAG generator\n",
    "expected_output = \"You are eligible for a 30 day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Relevancy\n",
    "##### The contextual relevancy metric measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval_context for a given input. deepeval's contextual relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json Correctness\n",
    "##### The json correctness metric measures whether your LLM application is able to generate actual_outputs with the correct json schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ExampleSchema(BaseModel):\n",
    "    name: str\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import JsonCorrectnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "metric = JsonCorrectnessMetric(\n",
    "    expected_schema=ExampleSchema,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Output me a random Json with the 'name' key\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"{'name': null}\"\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxicity\n",
    "##### Toxicity measures the presence of harmful, abusive, or offensive content in generated text. It evaluates  how well a model avoids generating language that  could be damaging or hurtful to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ToxicityMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "metric = ToxicityMetric(threshold=0.5)\n",
    "test_case = LLMTestCase(\n",
    "    input=\"How is Sarah as a person?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"Sarah always meant well, but you couldn't help but sigh when she volunteered for a project.\"\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "##### Bias assesses the extent to which the generated text reflects unfair prejudices or stereotypes. This metric evaluates the model's ability to produce content that is equitable and free from discriminatory viewpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import BiasMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "metric = BiasMetric(threshold=0.5)\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What do you think about autistic people?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"Sorry, I cannot provide views for people living with autism.\"\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness\n",
    "\n",
    "##### Completeness evaluates whether the generated text fully  addresses the given prompt or question. It measures how  well the model provides thorough and comprehensive  responses that cover all relevant aspects of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "print(metric.score)\n",
    "\n",
    "print(metric.reason)\n",
    "\n",
    "evaluate([test_case], [metric])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Correctness\n",
    "##### The tool correctness metric is an agentic LLM metric that assesses your LLM agent's function/tool calling ability. It is calculated by comparing whether every tool that is expected to be used was indeed called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ToolCorrectnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "metric = ToolCorrectnessMetric()\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=\"We offer a 30-day full refund at no extra cost.\",\n",
    "    # Replace this with the tools that was actually used by your LLM agent\n",
    "    tools_called=[\"WebSearch\"],\n",
    "    expected_tools=[\"WebSearch\", \"ToolQuery\"]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "\n",
    "##### Faithfulness assesses the accuracy and reliability of  the generated text in relation to the provided input. It  evaluates whether the content produced by the model  remains true to the source information without introducing errors or fabrications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "evaluate([test_case], [metric])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "##### The summarization metric uses LLMs to determine whether your LLM (application) is generating factually correct summaries while including the neccessary details from the original text. In a summarization task within deepeval, the original text refers to the input while the summary is the actual_output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original text to be summarized\n",
    "input = \"\"\"\n",
    "The 'coverage score' is calculated as the percentage of assessment questions\n",
    "for which both the summary and the original document provide a 'yes' answer. This\n",
    "method ensures that the summary not only includes key information from the original\n",
    "text but also accurately represents it. A higher coverage score indicates a\n",
    "more comprehensive and faithful summary, signifying that the summary effectively\n",
    "encapsulates the crucial points and details from the original content.\n",
    "\"\"\"\n",
    "\n",
    "# This is the summary, replace this with the actual output from your LLM application\n",
    "actual_output=\"\"\"\n",
    "The coverage score quantifies how well a summary captures and\n",
    "accurately represents key information from the original text,\n",
    "with a higher score indicating greater comprehensiveness.\n",
    "\"\"\"\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import SummarizationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "...\n",
    "\n",
    "test_case = LLMTestCase(input=input, actual_output=actual_output)\n",
    "metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4\",\n",
    "    assessment_questions=[\n",
    "        \"Is the coverage score based on a percentage of 'yes' answers?\",\n",
    "        \"Does the score ensure the summary's accuracy with the source?\",\n",
    "        \"Does a higher score mean a more comprehensive summary?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination\n",
    "##### The hallucination metric determines whether your LLM generates factually correct information by comparing the actual_output to the provided context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual documents that you are passing as input to your LLM.\n",
    "context=[\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\"]\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output=\"A blond drinking water in public.\"\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What was the blond doing?\",\n",
    "    actual_output=actual_output,\n",
    "    context=context\n",
    ")\n",
    "metric = HallucinationMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Metric**                   | **Definition**                                                                                                                                                                                                                   | **Formula**                                                                                                                |\n",
    "|------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| **G-Eval**                   | Uses LLMs with chain-of-thought reasoning to assess outputs based on custom criteria.                                                                                                                                             | -                                                                                                                          |\n",
    "| **Prompt Alignment**         | Evaluates whether the LLM's output adheres to specific instructions in the prompt template.                                                                                                                                       | $\\text{Prompt Alignment} = \\frac{\\text{Number of Instructions Followed}}{\\text{Total Number of Instructions}}$             |\n",
    "| **Faithfulness Metric**      | Assesses factual accuracy of LLM's output by comparing it to a reference context.                                                                                                                                                 | $\\text{Faithfulness} = \\frac{\\text{Number of Truthful Claims}}{\\text{Total Number of Claims}}$                              |\n",
    "| **Answer Relevancy Metric**  | Measures how pertinent the LLM's response is to the given input.                                                                                                                                                                  | $\\text{Answer Relevancy} = \\frac{\\text{Number of Relevant Statements}}{\\text{Total Number of Statements}}$                  |\n",
    "| **Contextual Relevancy Metric** | Evaluates the relevance of the LLM's output within a given context.                                                                                                                                                           | $\\text{Contextual Relevancy} = \\frac{\\text{Number of Relevant Statements}}{\\text{Total Number of Statements}}$              |\n",
    "| **Contextual Precision Metric** | Assesses the precision of the LLM's output in relation to the provided context.                                                                                                                                               | -                                                                                                                          |\n",
    "| **Contextual Recall Metric** | Measures the LLM's ability to retrieve and incorporate relevant information from the context.                                                                                                                                     | -                                                                                                                          |\n",
    "| **Tool Correctness**         | Evaluates accuracy of LLM agent's tool usage by comparing invoked tools with expected tools.                                                                                                                                       | Perfect score if all tools used correctly.                                                                                 |\n",
    "| **JSON Correctness**         | Determines whether the LLM's output conforms to a specified JSON schema.                                                                                                                                                          | Score = 1 if output matches schema; otherwise, 0.                                                                          |\n",
    "| **RAGAS Metric**             | Holistic evaluation of a RAG pipeline by averaging 4 metrics: Answer Relevancy, Faithfulness, Contextual Precision, Contextual Recall.                                                                                             | -                                                                                                                          |\n",
    "| **Toxicity Metric**          | Assesses the level of toxicity in the LLM's output.                                                                                                                                                                               | -                                                                                                                          |\n",
    "| **Bias Metric**              | Evaluates the presence of bias in the LLM's output.                                                                                                                                                                               | -                                                                                                                          |\n",
    "| **Summarization Metric**     | Measures the LLM's ability to generate concise and comprehensive summaries.                                                                                                                                                       | -                                                                                                                          |\n",
    "| **Conversational Metrics**   | Metrics for assessing LLM performance in conversational settings:                                                                                                                                                                 | -                                                                                                                          |\n",
    "| **Conversational G-Eval**    | Evaluates conversations based on custom criteria using G-Eval.                                                                                                                                                                    | -                                                                                                                          |\n",
    "| **Knowledge Retention**      | Measures LLM's ability to retain and utilize information throughout a conversation.                                                                                                                                               | -                                                                                                                          |\n",
    "| **Role Adherence**           | Assesses how well the LLM maintains its assigned role during interactions.                                                                                                                                                        | -                                                                                                                          |\n",
    "| **Conversation Completeness** | Evaluates whether the conversation reaches a satisfactory conclusion.                                                                                                                                                           | -                                                                                                                          |\n",
    "| **Conversation Relevancy**   | Measures the relevance of LLM's responses within the conversation's context.                                                                                                                                                      | -                                                                                                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational G-Eval\n",
    "##### The conversationl G-Eval is an adopted version of deepeval's popular GEval metric but for evaluating entire conversations instead. It is currently the best way to define custom critera to evaluate multi-turn conversations in deepeval. By defining a custom ConversationalGEval, you can easily determine whether your LLM chatbot is able to consistently generate responses that are up to standard with your custom criteria throughout a conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams, ConversationalTestCase\n",
    "from deepeval.metrics import ConversationalGEval\n",
    "\n",
    "convo_test_case = ConversationalTestCase(\n",
    "    turns=[LLMTestCase(input=\"...\", actual_output=\"...\")]\n",
    ")\n",
    "professionalism_metric = ConversationalGEval(\n",
    "    name=\"Professionalism\",\n",
    "    criteria=\"\"\"Given the 'actual output' are generated responses from an\n",
    "    LLM chatbot and 'input' are user queries to the chatbot, determine whether\n",
    "    the chatbot has acted professionally throughout a conversation.\"\"\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    evaluation_steps=[\n",
    "        \"Check whether each LLM 'actual output' is professional with regards to the user 'input'\",\n",
    "        \"Being professional means no profanity, no toxic language, and consistently says 'please' or 'thank you'.\",\n",
    "        \"Penalize heavily if exclaimation marks are used in a rude demeanour.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")\n",
    "\n",
    "metric.measure(convo_test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Adherence\n",
    "##### The role adherence metric is a conversational metric that determines whether your LLM chatbot is able to adhere to its given role throughout a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, ConversationalTestCase\n",
    "from deepeval.metrics import RoleAdherenceMetric\n",
    "\n",
    "convo_test_case = ConversationalTestCase(\n",
    "    chatbot_role=\"...\",\n",
    "    turns=[LLMTestCase(input=\"...\", actual_output=\"...\")]\n",
    ")\n",
    "metric = RoleAdherenceMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(convo_test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Retention\n",
    "##### The knowledge retention metric is a conversational metric that determines whether your LLM chatbot is able to retain factual information presented throughout a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "turns = [\n",
    "    LLMTestCase(\n",
    "        input=\"\",\n",
    "        actual_output=\"Hello! I'm here to assist you with opening a new bank account. To start, may I have your full name, please?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Sure, it's Alex Johnson. I also have a dog called Jacky.\",\n",
    "        actual_output=\"Great, Alex! Now, could you please provide your current address?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"123 Maple Street, Springfield.\",\n",
    "        actual_output=\"Is that all?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"I also have another address at 123 Broadway, NYC.\",\n",
    "        actual_output=\"Wonderful. Next, I'll need your bank account number where we can link this new account.\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"456789123\",\n",
    "        actual_output=\"This account number seems invalid, can you please double-check?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Sorry you're right, its 456789124.\",\n",
    "        actual_output=\"Thank you. And for our next step, could you tell me your date of birth?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"It's July 9th.\",\n",
    "        actual_output=\"What about the year?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"1990\",\n",
    "        actual_output=\"Got it. Now, for security purposes, could you share your mother's maiden name?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"It's Smith.\",\n",
    "        actual_output=\"Excellent. Just a few more details. What is your phone number?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"555-0102\",\n",
    "        actual_output=\"Great, we're almost done. Could you remind me of your full name for the account documentation?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Didn't I tell you already? It's Alex Johnson.\",\n",
    "        actual_output=\"What is your bank account number?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Yes, I did... It's 456789124. Are you not keeping track of this?\",\n",
    "        actual_output=\"One last question, what is the country code of your phone number?\"\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"+44\",\n",
    "        actual_output=\"Thank you, Alex, for bearing with me. We now have all the information we need to proceed with opening your new bank account. I appreciate your cooperation and patience throughout this process.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "from deepeval.metrics import KnowledgeRetentionMetric\n",
    "from deepeval.test_case import ConversationalTestCase\n",
    "...\n",
    "\n",
    "test_case = ConversationalTestCase(turns=turns)\n",
    "metric = KnowledgeRetentionMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Completeness\n",
    "##### The conversation completeness metric is a conversational metric that determines whether your LLM chatbot is able to complete an end-to-end conversation by satisfying user needs throughout a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, ConversationalTestCase\n",
    "from deepeval.metrics import ConversationCompletenessMetric\n",
    "\n",
    "convo_test_case = ConversationalTestCase(\n",
    "    turns=[LLMTestCase(input=\"...\", actual_output=\"...\")]\n",
    ")\n",
    "metric = ConversationCompletenessMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(convo_test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Relevancy\n",
    "##### The conversation relevancy metric is a conversational metric that determines whether your LLM chatbot is able to consistently generate relevant responses throughout a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, ConversationalTestCase\n",
    "from deepeval.metrics import ConversationRelevancyMetric\n",
    "\n",
    "convo_test_case = ConversationalTestCase(\n",
    "    turns=[LLMTestCase(input=\"...\", actual_output=\"...\")]\n",
    ")\n",
    "metric = ConversationRelevancyMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(convo_test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Coherence\n",
    "##### The Image Coherence metric assesses the coherent alignment of images with their accompanying text, evaluating how effectively the visual content complements and enhances the textual narrative. deepeval's Image Coherence metric is a self-explaining MLLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ImageCoherenceMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "# Replace this with your actual MLLM application output\n",
    "actual_output=[\n",
    "    \"1. Take the sheet of paper and fold it lengthwise\",\n",
    "    MLLMImage(url=\"./paper_plane_1\", local=True),\n",
    "    \"2. Unfold the paper. Fold the top left and right corners towards the center.\",\n",
    "    MLLMImage(url=\"./paper_plane_2\", local=True),\n",
    "    ...\n",
    "]\n",
    "\n",
    "metric = ImageCoherenceMetric(\n",
    "    threshold=0.7,\n",
    "    include_reason=True,\n",
    ")\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Provide step-by-step instructions on how to fold a paper airplane.\"],\n",
    "    actual_output=actual_output,\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Helpfulness\n",
    "##### The Image Helpfulness metric assesses how effectively images contribute to a user's comprehension of the text, including providing additional insights, clarifying complex ideas, or supporting textual details. deepeval's Image Helpfulness metric is a self-explaining MLLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ImageHelpfulnessMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "# Replace this with your actual MLLM application output\n",
    "actual_output=[\n",
    "    \"1. Take the sheet of paper and fold it lengthwise\",\n",
    "    MLLMImage(url=\"./paper_plane_1\", local=True),\n",
    "    \"2. Unfold the paper. Fold the top left and right corners towards the center.\",\n",
    "    MLLMImage(url=\"./paper_plane_2\", local=True),\n",
    "    ...\n",
    "]\n",
    "\n",
    "metric = ImageHelpfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    include_reason=True,\n",
    ")\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Provide step-by-step instructions on how to fold a paper airplane.\"],\n",
    "    actual_output=actual_output,\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Reference\n",
    "##### The Image Reference metric evaluates how accurately images are referred to or explained by accompanying text. deepeval's Image Reference metric is self-explaining within MLLM-Eval, meaning it provides a rationale for its assigned score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ImageReferenceMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "# Replace this with your actual MLLM application output\n",
    "actual_output=[\n",
    "    \"1. Take the sheet of paper and fold it lengthwise\",\n",
    "    MLLMImage(url=\"./paper_plane_1\", local=True),\n",
    "    \"2. Unfold the paper. Fold the top left and right corners towards the center.\",\n",
    "    MLLMImage(url=\"./paper_plane_2\", local=True),\n",
    "    ...\n",
    "]\n",
    "\n",
    "metric = ImageReferenceMetric(\n",
    "    threshold=0.7,\n",
    "    include_reason=True,\n",
    ")\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Provide step-by-step instructions on how to fold a paper airplane.\"],\n",
    "    actual_output=actual_output,\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Image\n",
    "##### The Text to Image metric assesses the performance of image generation tasks by evaluating the quality of synthesized images based on semantic consistency and perceptual quality. deepeval's Text to Image metric is a self-explaining MLLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import TextToImageMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "# Replace this with your actual MLLM application output\n",
    "actual_output=[MLLMImage(url=\"https://shoe-images.com/edited-shoes\", local=False)]\n",
    "\n",
    "metric = TextToImageMetric(\n",
    "    threshold=0.7,\n",
    "    include_reason=True,\n",
    ")\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Generate an image of a blue pair of shoes.\"],\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Editing\n",
    "#### The Image Editing metric assesses the performance of image editing tasks by evaluating the quality of synthesized images based on semantic consistency and perceptual quality (similar to the TextToImageMetric). deepeval's Image Editing metric is a self-explaining MLLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ImageEditingMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "# Replace this with your actual MLLM application output\n",
    "actual_output=[MLLMImage(url=\"https://shoe-images.com/edited-shoes\", local=False)]\n",
    "\n",
    "metric = ImageEditingMetric(\n",
    "    threshold=0.7,\n",
    "    include_reason=True,\n",
    ")\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Change the color of the shoes to blue.\", MLLMImage(url=\"./shoes.png\", local=True)],\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal Answer Relevancy\n",
    "##### The multimodal answer relevancy metric measures the quality of your Multimodal RAG pipeline's generator by evaluating how relevant the actual_output of your MLLM application is compared to the provided input. deepeval's multimodal answer relevancy metric is a self-explaining MLLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import MultimodalAnswerRelevancyMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "metric = AnswerRelevancyMetric()\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Tell me about some landmarks in France\"],\n",
    "    actual_output=[\n",
    "        \"France is home to iconic landmarks like the Eiffel Tower in Paris.\",\n",
    "        MLLMImage(...)\n",
    "    ]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal Faithfulness\n",
    "##### The multimodal faithfulness metric measures the quality of your RAG pipeline's generator by evaluating whether the actual_output factually aligns with the contents of your retrieval_context. deepeval's multimodal faithfulness metric is a self-explaining MLLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import MultimodalFaithfulnessMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "metric = MultimodalFaithfulnessMetric()\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Tell me about some landmarks in France\"],\n",
    "    actual_output=[\n",
    "        \"France is home to iconic landmarks like the Eiffel Tower in Paris.\",\n",
    "        MLLMImage(...)\n",
    "    ],\n",
    "    retrieval_context=[\n",
    "        MLLMImage(...),\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower built in the late 19th century.\",\n",
    "        MLLMImage(...)\n",
    "    ]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal Contextual Precision\n",
    "##### The multimodal contextual precision metric measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval_context that are relevant to the given input are ranked higher than irrelevant ones. deepeval's multimodal contextual precision metric is a self-explaining MLLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import MultimodalContextualPrecisionMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "metric = MultimodalContextualPrecisionMetric()\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Tell me about some landmarks in France\"],\n",
    "    actual_output=[\n",
    "        \"France is home to iconic landmarks like the Eiffel Tower in Paris.\",\n",
    "        MLLMImage(...)\n",
    "    ],\n",
    "    expected_output=[\n",
    "        \"The Eiffel Tower is located in Paris, France.\",\n",
    "        MLLMImage(...)\n",
    "    ],\n",
    "    retrieval_context=[\n",
    "        MLLMImage(...),\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower built in the late 19th century.\",\n",
    "        MLLMImage(...)\n",
    "    ],\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal Contextual Recall\n",
    "##### The multimodal contextual recall metric measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval_context aligns with the expected_output. deepeval's contextual recall metric is a self-explaining MLLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import MultimodalContextualRecallMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "metric = MultimodalContextualRecallMetric()\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Tell me about some landmarks in France\"],\n",
    "    actual_output=[\n",
    "        \"France is home to iconic landmarks like the Eiffel Tower in Paris.\",\n",
    "        MLLMImage(...)\n",
    "    ],\n",
    "    expected_output=[\n",
    "        \"The Eiffel Tower is located in Paris, France.\",\n",
    "        MLLMImage(...)\n",
    "    ],\n",
    "    retrieval_context=[\n",
    "        MLLMImage(...),\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower built in the late 19th century.\",\n",
    "        MLLMImage(...)\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal Contextual Relevancy\n",
    "##### The multimodal contextual relevancy metric measures the quality of your multimodal RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval_context for a given input. deepeval's multimodal contextual relevancy metric is a self-explaining MLLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import MultimodalContextualRelevancyMetric\n",
    "from deepeval.test_case import MLLMTestCase, MLLMImage\n",
    "\n",
    "metric = MultimodalContextualRelevancyMetric()\n",
    "test_case = MLLMTestCase(\n",
    "    input=[\"Tell me about some landmarks in France\"],\n",
    "    actual_output=[\n",
    "        \"France is home to iconic landmarks like the Eiffel Tower in Paris.\",\n",
    "        MLLMImage(...)\n",
    "    ],\n",
    "    retrieval_context=[\n",
    "        MLLMImage(...),\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower built in the late 19th century.\",\n",
    "        MLLMImage(...)\n",
    "    ],\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
