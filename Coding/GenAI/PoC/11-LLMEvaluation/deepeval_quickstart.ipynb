{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDgDprF_xbkS"
      },
      "source": [
        "# ðŸ¤— Welcome to DeepEval!\n",
        "\n",
        "Thanks for trying us out, we're here to provide you with the best LLM evaluation experience you can dream of ðŸ˜Š any questions or concerns you may have, [come talk to us on discord,](https://discord.com/invite/a3K9c8GRGt) we're always here to help!\n",
        "\n",
        "# What is DeepEval?\n",
        "\n",
        "DeepEval is the evaluation framework for LLMs. It takes the latest research (eg., G-Eval, SelfCheckGPT, RAGAS) in LLM evaluation and implement it as metrics for anyone to easily plug and use.\n",
        "\n",
        "Our strength lies in the simplicity and ease of use, while being a full evaluation suite for LLMs. Hope you enjoy trying us out!\n",
        "\n",
        "# Installation\n",
        "\n",
        "Install deepeval, you can ignore the warnings at the end of the installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz58Byh2m60V"
      },
      "outputs": [],
      "source": [
        "!pip install -U deepeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8weaJmIDr1PF"
      },
      "source": [
        "^^ Please ignore the warnings after installation, but if you're concerned please run `!pip install cohere` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMwb2vPYoA2Y"
      },
      "source": [
        "# Login (recommended step)\n",
        "\n",
        "Login to Confident AI to log evaluation results on the cloud. Later, you can use Confident AI to centralize evaluation datasets, perform real-time evaluations in production, and much more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1Xh8ybXn7dJ"
      },
      "outputs": [],
      "source": [
        "!deepeval login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoyW1ntQoQbv"
      },
      "source": [
        " # ðŸ˜‡ Create Your First Test Case\n",
        "\n",
        " A test case in `deepeval` mimics a user interaction with your LLM (application)Here:\n",
        " - `input` is what a user would input\n",
        " - `actual_output` is the output of yoru LLM (application)\n",
        " - `retrieval_context` is the retrieved context in your RAG pipeline.\n",
        "\n",
        "Note that only RAG metrics require `retrieval_context` when creating a test case. Visit the [test cases section in our docs](https://docs.confident-ai.com/docs/evaluation-test-cases) to learn about how a test case work in `deepeval`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGjEshHFm8tD"
      },
      "outputs": [],
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "  input=\"What if these shoes don't fit?\",\n",
        "  # Replace this with the actual output of your LLM application\n",
        "  actual_output=\"We offer a 30-day full refund at no extra cost.\",\n",
        "  # Replace this with the retrieval context (in the RAG pipeline) of your LLM application\n",
        "  retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSlhH0KpGk-"
      },
      "source": [
        "# ðŸ¤— Create Your First Metric\n",
        "\n",
        "A metric in `deepeval` evaluates based on the values of the parameters in a particular test case. `deepeval` incorporates the latest research into its metrics implementation so you don't have to.\n",
        "\n",
        "## Set OpenAI API Key\n",
        "\n",
        "Most of `deepeval`'s metrics are evaluated using LLMs. To begin, set your `OPENAI_API_KEY` below (IMPORTANT: don't include quotation \"\" marks!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMwsf9mfsZWU"
      },
      "outputs": [],
      "source": [
        "%env OPENAI_API_KEY=<your-openai-api-key>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksQJRtrskG3"
      },
      "source": [
        "(Note that you don't have to use OpenAI, although we highly highly highly recommend it, since evaluation requires a high level of reasoning. That being said, if you want to use a custom model like Azure OpenAI, visit the [metrics section in our docs](https://docs.confident-ai.com/docs/metrics-introduction#using-a-custom-llm))\n",
        "\n",
        "\n",
        "## Create Your Metric\n",
        "\n",
        "In this example, we create an `AnswerRelevancyMetric`, which measures the answer relevancy of a RAG based LLM application. Not all metrics are RAG metrics. For a list of full metrics and an explanation for each, visit [the metrics section in our docs](https://docs.confident-ai.com/docs/metrics-introduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9HpXp6sngFu"
      },
      "outputs": [],
      "source": [
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v2tKr8Yp8Ry"
      },
      "source": [
        "# ðŸš€ Run Your First Evaluation\n",
        "\n",
        "With a test case and metric ready, you can start using our `evaluate()` function to evaluate your LLM (application).\n",
        "\n",
        "The `evaluate()` function accepts a list of test cases, and a list of metrics. Under the hood, it evaluates each individual test case using the list of provided metrics. A test case only passess if all the metrics are passing. For more information, including how to use our Pytest integration for evaluation, visit the evaluation [section in our docs.](https://docs.confident-ai.com/docs/evaluation-introduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0yBxUBHnzWF"
      },
      "outputs": [],
      "source": [
        "from deepeval import evaluate\n",
        "\n",
        "evaluate([test_case], [answer_relevancy_metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhU0L9uWrWk1"
      },
      "source": [
        "# Using Standalone Metrics\n",
        "\n",
        "`deepeval` offers a simple way for anyone to plug and use our metrics. This is especially useful if you're looking to build your own evaluation pipelines. Using the previous example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaWouoOUrkgj"
      },
      "outputs": [],
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "  input=\"What if these shoes don't fit?\",\n",
        "  # Replace this with the actual output of your LLM application\n",
        "  actual_output=\"We offer a 30-day full refund at no extra cost.\",\n",
        "  # Replace this with the retrieval context (in the RAG pipeline) of your LLM application\n",
        "  retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
        ")\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "answer_relevancy_metric.measure(test_case)\n",
        "print(answer_relevancy_metric.score)\n",
        "print(answer_relevancy_metric.reason)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPh31S8QuCam"
      },
      "source": [
        "# Create Your First Evaluation Dataset\n",
        "\n",
        "An evaluation dataset in `deepeval` is a collection of test cases. It offers a rich set of features for you to manipulate evaluation data. For more information, visit the [dataset section in our docs.](https://docs.confident-ai.com/docs/evaluation-datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxFMHQZOucxs"
      },
      "outputs": [],
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "\n",
        "test_case_1 = LLMTestCase(\n",
        "  input=\"What if these shoes don't fit?\",\n",
        "  actual_output=\"We offer a 30-day full refund at no extra cost.\",\n",
        "  retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
        ")\n",
        "test_case_2 = LLMTestCase(\n",
        "  input=\"What should you do if there's a fire?\",\n",
        "  actual_output=\"Drop and roll.\",\n",
        "  retrieval_context=[\"Don't use the elevator in the case of a fire.\"]\n",
        ")\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "dataset = EvaluationDataset(test_cases=[test_case_1, test_case_2])\n",
        "\n",
        "# Same as before, using the evaluate function\n",
        "evaluate(dataset, [answer_relevancy_metric])\n",
        "\n",
        "# Or, use the evaluate method directly, they're exactly the same\n",
        "# dataset.evaluate([answer_relevancy_metric])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
