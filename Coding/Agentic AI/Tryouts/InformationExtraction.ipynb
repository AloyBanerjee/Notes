{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os \n",
    "from phi.model.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        default=None, description=\"The color of the person's hair if known\"\n",
    "    )\n",
    "    height_in_meters: Optional[str] = Field(\n",
    "        default=None, description=\"Height measured in meters\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "\n",
    "    # Creates a model so that we can extract multiple entities.\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        # Please see the how-to about improving performance with\n",
    "        # reference examples.\n",
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\") #Groq(id ='llama-3.2-3b-preview')#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pet(BaseModel):\n",
    "  name: str\n",
    "  animal: str\n",
    "  age: int\n",
    "  color: str | None\n",
    "  favorite_toy: str | None\n",
    "\n",
    "class PetList(BaseModel):\n",
    "  pets: list[Pet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_text = '''\n",
    "        I have two pets.\n",
    "        A cat named Luna who is 5 years old and loves playing with yarn. She has grey fur.\n",
    "        I also have a 2 year old black cat named Loki who loves tennis balls.\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': information_text,\n",
    "    }\n",
    "  ],\n",
    "  model='llama3.1',#ollama run llama3.1\n",
    "  format=PetList.model_json_schema(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pets=[Pet(name='Luna', animal='cat', age=5, color='grey', favorite_toy='yarn'), Pet(name='Loki', animal='cat', age=2, color='black', favorite_toy='tennis ball')]\n"
     ]
    }
   ],
   "source": [
    "pets = PetList.model_validate_json(response.message.content)\n",
    "print(pets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction for dropdown list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import LLMChain\n",
    "from ollama import chat\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10440\\398291582.py:51: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10440\\398291582.py:54: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run({\"paragraph\": paragraph})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PersonList': [{'Name': 'Clara', 'Height': '5\\'6\"', 'HairColor': 'Auburn', 'hair_color': 'unknown'}, {'Name': 'Mark', 'Height': '6\\'3\"', 'HairColor': 'Jet-black', 'hair_color': 'unknown'}, {'Name': 'Mia', 'Height': '5\\'2\"', 'HairColor': 'Platinum-blonde', 'hair_color': 'unknown'}]}\n"
     ]
    }
   ],
   "source": [
    "# Define response schemas\n",
    "independent_schema = [\n",
    "    ResponseSchema(name=\"name\", description=\"The name of the person\"),\n",
    "    ResponseSchema(\n",
    "        name=\"hair_color\",\n",
    "        description=\"The color of the person's hair if known (grey, black, white, or unknown)\",\n",
    "        enum=[\"grey\", \"black\", \"white\", \"unknown\"]\n",
    "    ),\n",
    "    ResponseSchema(name=\"HairColor\", description=\"The color of the person's hair if known\"),\n",
    "    ResponseSchema(name=\"height_in_meters\", description=\"Height measured in meters\"),\n",
    "]\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"PersonList\",\n",
    "        description=\"A list of all persons with their details\",\n",
    "        type=\"array\",\n",
    "        items={\"type\": \"object\", \"properties\": independent_schema},\n",
    "    )\n",
    "]\n",
    "\n",
    "# Parse the example output\n",
    "class PersonListParser(BaseOutputParser):\n",
    "    def parse(self, text: dict) -> dict:\n",
    "        # Safeguard: Ensure 'hair_color' exists and adjust it if needed\n",
    "        for person in text.get(\"PersonList\", []):\n",
    "            if \"hair_color\" not in person or person[\"hair_color\"] not in [\"grey\", \"black\", \"white\"]:\n",
    "                person[\"hair_color\"] = \"unknown\"\n",
    "        return text  # Return the adjusted structured data\n",
    "\n",
    "# Create an output parser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extract the information from the given context.\\n{format_instructions}\\n\\nContext: {paragraph}\",\n",
    "    input_variables=[\"paragraph\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# Example paragraph\n",
    "paragraph = '''The warm sun spilled through the tall oak trees as the forest glade came alive with the sound of laughter. \n",
    "            Clara, her auburn hair catching the golden light, stood at the edge of the brook, her 5'6\" frame bent slightly as she skipped a stone across the water. \n",
    "            Beside her, Mark towered at 6'3\", his jet-black hair ruffled by the breeze, giving him the appearance of someone caught between seriousness and mischief. \n",
    "            \"Try this one,\" he said, handing Clara a perfectly flat pebble, his voice carrying the easy confidence of someone who always knew what to do. \n",
    "            In the background, their friend Mia, petite at just 5'2\" with an unmistakable cascade of platinum-blonde curls, leaned against a tree trunk, \n",
    "            sketching the scene in her notebook with quiet focus. \n",
    "            The trio seemed at home in the serene wilderness, their contrasting heights and features painting a vivid picture of camaraderie.'''\n",
    "\n",
    "# Combine the prompt and LLM into a chain\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Run the chain with the paragraph\n",
    "result = chain.run({\"paragraph\": paragraph})\n",
    "\n",
    "# Parse the output using the custom output parser\n",
    "parser = PersonListParser()\n",
    "parsed_output = parser.parse(output_parser.parse(result))\n",
    "\n",
    "print(parsed_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PersonList': [{'Name': 'Clara', 'Height': '5\\'6\"', 'HairColor': 'Auburn', 'hairColor': 'unknown'}, {'Name': 'Mark', 'Height': '6\\'3\"', 'HairColor': 'Jet-Black', 'hairColor': 'unknown'}, {'Name': 'Mia', 'Height': '5\\'2\"', 'HairColor': 'Platinum-Blonde', 'hairColor': 'unknown'}]}\n"
     ]
    }
   ],
   "source": [
    "# Define response schemas\n",
    "independent_schema = [\n",
    "    ResponseSchema(name=\"name\", description=\"The name of the person\"),\n",
    "    ResponseSchema(name=\"hairColor\", description=\"The color of the person's hair if known\", enum=[\"grey\", \"black\", \"white\", \"unknown\"]),\n",
    "    ResponseSchema(name=\"HairColor\", description=\"The color of the person's hair if known\"),\n",
    "    ResponseSchema(name=\"height_in_meters\", description=\"Height measured in meters\")\n",
    "]\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"PersonList\",\n",
    "        description=\"A list of all persons with their details\",\n",
    "        type=\"array\",\n",
    "        items={\"type\": \"object\", \"properties\": independent_schema},\n",
    "    )\n",
    "]\n",
    "\n",
    "# Parse the example output\n",
    "# class PetListParser(BaseOutputParser):\n",
    "#     def parse(self, text: dict) -> dict:\n",
    "#         return text  # Just returning the text as structured data\n",
    "\n",
    "# Parse the example output\n",
    "class PersonListParser(BaseOutputParser):\n",
    "    def parse(self, text: dict) -> dict:\n",
    "        # Adjust the hair color field to \"unknown\" if it is not in the enum\n",
    "        for person in text.get(\"PersonList\", []):\n",
    "              if \"hairColor\" not in person or person[\"hairColor\"] not in [\"grey\", \"black\", \"white\"]:\n",
    "                person[\"hairColor\"] = \"unknown\"\n",
    "        return text  # Returning the adjusted structured data\n",
    "\n",
    "# Create an output parser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extract the information from the given context.\\n{format_instructions}\\n\\nContext: {paragraph}\",\n",
    "    input_variables=[\"paragraph\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# Example paragraph\n",
    "paragraph = '''The warm sun spilled through the tall oak trees as the forest glade came alive with the sound of laughter. \n",
    "            Clara, her auburn hair catching the golden light, stood at the edge of the brook, her 5'6\" frame bent slightly as she skipped a stone across the water. \n",
    "            Beside her, Mark towered at 6'3\", his jet-black hair ruffled by the breeze, giving him the appearance of someone caught between seriousness and mischief. \n",
    "            \"Try this one,\" he said, handing Clara a perfectly flat pebble, his voice carrying the easy confidence of someone who always knew what to do. \n",
    "            In the background, their friend Mia, petite at just 5'2\" with an unmistakable cascade of platinum-blonde curls, leaned against a tree trunk, \n",
    "            sketching the scene in her notebook with quiet focus. \n",
    "            The trio seemed at home in the serene wilderness, their contrasting heights and features painting a vivid picture of camaraderie.'''\n",
    "\n",
    "\n",
    "# Combine the prompt and LLM into a chain\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Run the chain with the paragraph\n",
    "result = chain.run({\"paragraph\": paragraph})\n",
    "\n",
    "# Parse the output using the output parser\n",
    "parser = PersonListParser()\n",
    "parsed_output = parser.parse(output_parser.parse(result))\n",
    "\n",
    "\n",
    "print(parsed_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Kees Van Der Westen Speedster\",\n",
      "  \"price\": 14499,\n",
      "  \"features\": [\n",
      "    \"Dual boilers for brewing and steaming\",\n",
      "    \"PID temperature control for consistency\",\n",
      "    \"Unique pre-infusion system to enhance flavor extraction\",\n",
      "    \"Customizable aesthetics\",\n",
      "    \"Exceptional thermal stability\",\n",
      "    \"Intuitive operation via a lever system\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def parse_product(description: str) -> dict:\n",
    "    result = chain.invoke({\"input\": description})\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "output_parser_prod = JsonOutputParser(pydantic_object={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"price\": {\"type\": \"number\"},\n",
    "        \"features\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        }\n",
    "    }\n",
    "})\n",
    "prompt_prod = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Extract product details into JSON with this structure:\n",
    "        {{\n",
    "            \"name\": \"product name here\",\n",
    "            \"price\": number_here_without_currency_symbol,\n",
    "            \"features\": [\"feature1\", \"feature2\", \"feature3\"]\n",
    "        }}\"\"\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt_prod | llm | output_parser_prod\n",
    "\n",
    "\n",
    "description = \"\"\"The Kees Van Der Westen Speedster is a high-end, single-group espresso machine known for its precision, performance, \n",
    "and industrial design. Handcrafted in the Netherlands, it features dual boilers for brewing and steaming, PID temperature control for \n",
    "consistency, and a unique pre-infusion system to enhance flavor extraction. Designed for enthusiasts and professionals, it offers \n",
    "customizable aesthetics, exceptional thermal stability, and intuitive operation via a lever system. The pricing is approximatelyt $14,499 \n",
    "depending on the retailer and customization options.\"\"\"\n",
    "\n",
    "parse_product(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid return object. Expected key `hair_color` to be present, but got {'name': 'Kees Van Der Westen Speedster', 'price': 14499, 'features': ['Dual boilers for brewing and steaming', 'PID temperature control', 'Pre-infusion system', 'Customizable aesthetics', 'Exceptional thermal stability', 'Intuitive operation via a lever system']}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThe Kees Van Der Westen Speedster is a high-end, single-group espresso machine known for its precision, performance, \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mand industrial design. Handcrafted in the Netherlands, it features dual boilers for brewing and steaming, PID temperature control for \u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mconsistency, and a unique pre-infusion system to enhance flavor extraction. Designed for enthusiasts and professionals, it offers \u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mcustomizable aesthetics, exceptional thermal stability, and intuitive operation via a lever system. The pricing is approximatelyt $14,499 \u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mdepending on the retailer and customization options.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mparse_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m, in \u001b[0;36mparse_product\u001b[1;34m(description)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_product\u001b[39m(description: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(result, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32mh:\\Interview Preparation\\Coding\\Agentic AI\\Tryouts\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mh:\\Interview Preparation\\Coding\\Agentic AI\\Tryouts\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:193\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage],\n\u001b[0;32m    189\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m--> 193\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    203\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    205\u001b[0m             config,\n\u001b[0;32m    206\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    207\u001b[0m         )\n",
      "File \u001b[1;32mh:\\Interview Preparation\\Coding\\Agentic AI\\Tryouts\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1926\u001b[0m         Output,\n\u001b[1;32m-> 1927\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1935\u001b[0m     )\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mh:\\Interview Preparation\\Coding\\Agentic AI\\Tryouts\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Interview Preparation\\Coding\\Agentic AI\\Tryouts\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:194\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage],\n\u001b[0;32m    189\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m--> 194\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    197\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    198\u001b[0m             config,\n\u001b[0;32m    199\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m         )\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    203\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    205\u001b[0m             config,\n\u001b[0;32m    206\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    207\u001b[0m         )\n",
      "File \u001b[1;32mh:\\Interview Preparation\\Coding\\Agentic AI\\Tryouts\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:247\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: \u001b[38;5;28mlist\u001b[39m[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Interview Preparation\\Coding\\Agentic AI\\Tryouts\\venv\\Lib\\site-packages\\langchain\\output_parsers\\structured.py:97\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m     96\u001b[0m     expected_keys \u001b[38;5;241m=\u001b[39m [rs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_schemas]\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Interview Preparation\\Coding\\Agentic AI\\Tryouts\\venv\\Lib\\site-packages\\langchain_core\\utils\\json.py:190\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[1;34m(text, expected_keys)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_obj:\n\u001b[0;32m    186\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid return object. Expected key `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be present, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         )\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_obj\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Got invalid return object. Expected key `hair_color` to be present, but got {'name': 'Kees Van Der Westen Speedster', 'price': 14499, 'features': ['Dual boilers for brewing and steaming', 'PID temperature control', 'Pre-infusion system', 'Customizable aesthetics', 'Exceptional thermal stability', 'Intuitive operation via a lever system']}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Json and Object Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
